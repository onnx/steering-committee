## Steering Committee Meeting 11/18/2020

### Attendance:

| Name (Affiliation)              | Present  |
| ------------------------------- | -------- |
| Prasanth Pulavarthi (Microsoft) |    Yes   |
| Harry Kim (Intel)               |    Yes  |
| Jim Spohrer (IBM)               |    Yes   |
| Joohoon Lee (Nvidia)            |    Yes  |
| Sheng Zha (AWS)                 |      |

Others: Ti Zhou (Baidu), Thomas Troung

### Agenda:

* Invited SC talks (15 mins) 
     * Kuberflow-On-Prem discussion (Postponed until Dec)
     * Other potential topics? RedHat ODH etc. 
        * Jim: Use slack to coordinate and host invited talks. 

* 1.8 release
     * Thank you Jacky for leading and completing!
     * Chin: Bumps - release WG may not be needed until a month before next release (pause?)
     * Chin: Travis new URL website - slow travis.org, move to travis.com (free)
     * Chin: Same issues wih TensorFlow converter
     * Asking who can change the URL to travis.com - resolved
     * Prasanth: Connected with Chin => move to github actions in the future
     
* Workshop (China friendly) 
     * Waiting for response from Ke (Ant financial)
     * Ask using announce alias and on slack
     * Target date:  
     * LFAI requires 3 months lead-in time

* Roadmap upcoming discussions (Harry)
     * Request for brainstorming: New vision statement
     * Long term goals for ONNX: 
        * Increase adoption & awareness (Jim: link with performance. Zetane wants to support explainability of ONNX models via visualization)
        * Better UX 
     * Tactical items brainstorming: 
        * Enhanced integration with existing frameworks & smoother model exports
        * More models: User contributed zoo + official zoo (Examples: HuggingFace, HW NAS Search)
          * Prasanth: MLperf has matching ONNX models. HuggingFace has ONNX versions of their models. 
          * Joohoon: Update the old ONNX models in the model zoo
        * Ensure quality of ONNX models in Zoo (Models from papers with > 500 citations)
        * Decide on the criteria of "retiring" models
        * => More op coverage
        * Enhanced deployment support (e.g. model compression via QAT) 
        * Better traditional ML support (e.g. via hummingbird)
        * Preprocessing (Bring the support varied by domain)
        
        
        
