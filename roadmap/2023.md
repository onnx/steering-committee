# Roadmap for year 2023

Below are the summary of our roadmap discussions during year 2023.

## Topic 1: Operators

### [1] Double calculation on Tree Ensemble (Regressor & Classifier) ML operators.
Stefan Acin / 
Aizon	/
Operator SIG

Most of the operators already have a double floating point input and outputs. Some like the Linear Regressor do not, but can be simply recreated using a MatMul and Add nodes. TreeEnsembleRegressor and TreeEnsembleClassifier nodes are float32 only. The problems that arise from this are already documented in a few places across the sklearn2onnx documentation. In regulated and scientific industries, having double precision and getting the exact same results as provided by the sklearn models is critical for using ONNX models, even at the cost of speed gains by using only float32 operators. Feel free to contact me for more information. 

### [2] Additional beam search support and Dot Product Attention Op.
William Tambellini / 
RWS/LanguageWeaver	/
Operator SIG

For info about beam search, follow this [link](https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/)

Add a DotProductAttention Op, proposal [here](https://www.tutorialexample.com/an-introduction-to-scaled-dot-product-attention-in-deep-learning-deep-learning-tutorial/)

### [3] Additional OP DCNv2 in ONNX format.
Wei Wen	/ 
Intel	/
Operator SIG

New OP support - DCNv2	

### [4] Op support and usability improvements to ONNX for GNNs.
Ramakrishnan Sivakumar / 
Groq	/
Compiler SIG

Usage example: FP16 ONNX converter improvements: The FP16 converter, part of Onnx Converters Common in some cases generates invalid ONNX models or causes accuracy issues. Custom ONNX function registration with exporters: When export to ONNX is blocked due to missing op-support, the ability to register custom ONNX functions with onnx-exporters (e.g. torch.onnx) will enable the user to get a valid ONNX graph without waiting for the ONNX release cycle. This would improve the robustness of compilers with an ONNX front end.

## Topic 2: Operators and quantized operations.

### [5] Support for Hybrid FP8 and MHA operator	
Arnab Raha / 
Intel /
Operator SIG

It will be great to have support for FP8 formats - both HF8 and BF8 and a mixture of them based on the Intel numeric council recommendation. We also need to support MHA (Multi-Head Attention) as an operator as transformers are going to be the NN model of choice in the future.

### [6] Support for FP8 data formats.
Naveen Mellempudi / 
Intel 	/
Operator SIG

Adding support for FP8 data formats E5M2 and E4M3 is important for enabling inference on upcoming hardware platforms. There is conclusive data (we will be happy to present our internal data which is not yet public) that shows superiority compared to INT8 in terms of workload coverage across multiple domains of deep learning applications. We propose adopting the interchange format proposed in the joint specification from Arm, Intel and Nvidia. Link to the specification : https://arxiv.org/pdf/2209.05433.pdf 

### [7] Float16 support in quantizelinear and dequantizelinear.
Murali Ambati /	
Intel /
Operator SIG

Currently, float (FP32) is the only data-type supported for Quantizelinear (input) and dequantizelinear (output). The proposal is to include float16 data-type for these operators.


### [8] Adding bit parameter to QuantizeLinear and DequantizeLinear operator for improved hardware compatibility.
Lucas Fischer /	
Datakalab	/
Operator SIG

As hardware support for various quantization levels continues to expand, including binary neural networks on famous MCUs,  support from RISC-V and FPGA chip manufacturers for ternary and int4 quantization, as well as from some NPUs for int4 quantization, it is important to ensure compatibility with these hardware configurations. 

To address this, we propose adding a bit parameter to the QuantizeLinear and DequantizeLinear layers, and updating the related documentation to reflect the impact of saturation based on the selected bit value.

By default, the bit value would be set to 8 to ensure backward compatibility. 

This parameter would allow the expression of different quantization levels during deployment, if available on the target. By adding this parameter, we can ensure that the ONNX format remains compatible and flexible, promoting compatibility across different hardware configurations and encouraging innovation. 
Ultimately, this modification is expected to result in  improved integration and adoption.

### [9] Adopting a wider set of quantized ops into ONNX.
Peter van Beek, Aleksandar Sutic, Thomas Gardos	/ 
Intel	/
Operator SIG

Our goal is to use ONNX as file format for fully quantized models, to be consumed by a hardware inference toolchain.  Currently, we see only a very limited set of Q-ops defined in ONNX itself and a wider set defined in ONNX Runtime contrib.  We'd like to recommend to adopt a wider set of Q-ops into ONNX itself.  We'd like to achieve an end-to-end toolchain with basic CNN models for efficient embedded computer vision inference. 

## Topic 3

### [10] Distributed Inference and Communication Collectives. 
Ganesan Ramalingam	/ 
Microsoft	/
Operator SIG

As models grow larger, there is an increasing need to support model-inference using multiple GPUs (or CPUs). Commonly, such models use communication collectives, like AllGather, AllReduce, AllScatter. It would be useful to extend ONNX to support this scenario. Hopefully, this can be done by adding new ops, but this is a more significant change than the usual op addition.

### [11] Add a tokenization op.
W. Tambellini / 
RWS	/
Operator SIG (Pre/Post)

Rediscuss
[issue](https://github.com/onnx/onnx/issues/1474) and [PR](https://github.com/onnx/onnx/pull/1861)
without any Microsoft custom op. Add more support for Language Models

### [12]  Tokenizer / Support for Transformers.
Bourhan Dernayka / 
IBM 
Operator SIG (Pre/Post)

Today, the most used library to use a pre-trained language model (LM) is HuggingFace's Transformers. However, the ONNX support for such models is not optimal, especially when considering the model for inference. One of multiple suggestions that I can make is to include the tokenizer inside of the model.onnx, which cannot be done currently. This way, the ONNX model to extract will be ready to use standalone, without the need to tokenize the input sentences before inferencing (doing the prediction/classification).

### [13] Overhauling Training Info Proto for more comprehensive training information.
Taka Shinagawa / 
Microsoft	/ 
Training

"TrainingInfoProto was added to ONNX in 2019 so that users know how to apply the specified optimization algorithm to conduct further training iterations. However, in order to allow finetuning of ONNX models, ONNX files need to store more complete training information from upstream training checkpoints in many cases.  In addition, in the past few years, newer training frameworks such as DeepSpeed, Hugging Face Transformers and Nvidia Nemo are gaining popularity in addition to the fundamental Deep Learning and ML libraries such as PyTorch, Tensorflow, JAX, MXNet and Scikit-learn. Supporting checkpoint information for a wider variety of training framework is a new requirements with the emerging training frameworks.

The following are the main use cases for this feature:

* Finetuning of ONNX models. Without training information, we cannot finetune ONNX models and the downstream use cases are limited to inferencing. With ONNX files with more complete training information, it will be easier to convert ONNX files to training checkpoint/model formats for further training. This feature will allow us to utilize the ONNX Model Zoo for finetuning use cases in addition to inferencing use cases of existing models. 

* Model portability. By saving all training checkpoint information in an ONNX file, use cases for model portability across multiple training frameworks and checkpoint formats will be easier and more realistic. Model portability was one of the main goals of ONNX project from the inception. Given the current landscape of Deep Learning and Transformers/LLM frameworks including Hugging Face and DeepSpeed, updating TrainingInfoProto is essential for the model portability promise of ONNX Project.

* Model management. Today, data scientists and ML engineers need to maintain both training checkpoints and ONNX files for additional training and inferencing use cases because information needed for finetuning canâ€™t be stored in ONNX files. This situation complicates the model management and increases model storage overhead. By storing complete training information in ONNX files, we can make progress toward standardizing model management with ONNX files and facilitate the automation and streamlining of model management process.

* Model Conversion Tool Development. More comprehensive yet specific definition of TrainingInfoProto will make it easier for vendors and developers to create model conversion tools and libraries for a wider types of frameworks and model/checkpoint formats.

References:

*	Protos [ONNX 1.14.0 documentation](https://onnx.ai/onnx/api/classes.html#l-traininginfoproto)
*	ONNX Training Proposal by wschin. [Pull Request #2013](https://github.com/onnx/onnx/pull/2013)
*	Training Proposal: Spec Changes and Gradient Operator by wschin. [Pull Request #2314](https://github.com/onnx/onnx/pull/2314)

### [14] Web AI	GPU support for JavaScript.
Alexander Visheratin /		
Infra SIG
	
It would be great if the ONNX for Web could get better support for GPUs. Right now, the number of operators implemented for WebGL is rather limited, and from my experience, modern models for CV and NLP are not working with the WebGL backend. This greatly limits the usage of ONNX in browsers because, in web development, UX is crucial. And models running 60-90 seconds on the WASM backend are not quite UX-friendly.

## Topic 4

### [15] Multi-Head attention operation on ONNX
Alessandro Palla / 
Intel 	/
Operator SIG

The quick rise in popularity of LLM (Large-Language Models) made clear that hardware vendors need to prioritize and fine tune their hardware/kernels for this specific architecture. The most impactful layer of LLMs architecture in terms of performance is the Multi-Head Attention one. Also, those LLMs are made of the same basic building block replicated several times in the network, making the design very modular. Because of this, the return of investment on any MHA optimization is very high.

I propose therefore the introduction of a new ONNX operator for Multi-Head Attention (MHA) layer. This has been already introduced both in [PyTorch](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html) and [Tensorflow/Keras](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention).

I believe its introduction would bring several benefits for the ONNX project:

*	Simplified models: By having a multi-head attention operator in ONNX, the multiple atomic operators that made the MHA layer can be coalesced into one, making the models simpler and easier to manage, debug and visualize.
*	Increased popularity: as LLMs are gaining momentum and are now state-of-the-art, and their scope and popularity are expected to continue to grow, having a multi-head attention operator in ONNX would make it easier for developers to use these models and take advantage of their capabilities.
*	Standardization: a well-defined, standardized multi-head attention layer enables a more efficient mapping of the MHA layers onto the hardware. This will be key in enabling customers access to an efficient, performant implementation of MHA into hardware across various existing frameworks.
*	Improved performance: as multi-head attention is a key component of many LLMs, having a standardized, dedicated operator would allow hardware accelerators to tailor specific optimizations to this layer, improving therefore performance. 
*	Better hardware compatibility: Different multi-head attention implementations may have vastly different performance on hardware and having a dedicated operator in ONNX would help ensure that models are compatible with a wider range of hardware platforms.

I think therefore that an open-source project like ONNX will benefit greatly of having a specific Multi-Head Attention layer as a primitive operator.

### [16] Sparse Tensor Support.
Jacob Renn / 
AI Squared, Inc. /
Operator SIG

It would be extremely valuable to have a greater level of support for sparse tensors as weights for models within the ONNX ecosystem.  The LF AI and Data project [BeyondML](https://github.com/Beyond-ML-Labs/BeyondML) contains layers and utilities for utilizing sparse tensors in PyTorch and TensorFlow models to achieve more efficient model runtimes, but ONNX conversion is currently impossible due to the lack of support for sparse tensors as model weights.

### [17] Define operator attributes and add data-driven post-training sparsification capabilities.
Manuj Sabharwal, Ken Koyanagi / 
Intel	/
Operator SIG

We propose that operators are expanded beyond using Constant operator to indicate a sparse layer, maintaining an unstructured format for generality, which shall be easily distinguishable in the model graph. This should be supported in conversions/exports from TensorFlow and PyTorch, both of which support unstructured sparsity. In addition, **expand** upon the one-shot sparsification tool offered to also offer a data-driven iterative sparsification tool for post-training sparsification.  

### [18] Datetime parsing.
Christian Bourjau / 
QuantCo /
Operator SIG (pre/post)

One roadblock we have encountered several times in the past is the lack of datetime-to-unix-timestamp parsing. A node that works analogously to Python's [`strptime`](https://docs.python.org/3/library/datetime.html#datetime.datetime.strptime) would fill our use-case perfectly. Using `tensor(float64)` as the output type of that node would enable the use of `NaN` for malformed datetimes. 
